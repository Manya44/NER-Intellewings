# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14pUDPrHf9bYpI12mmN7MpVvRJxOBjPSz
"""

# Install necessary libraries
!pip install requests spacy nltk pandas sqlalchemy flask flask-restful gradio vaderSentiment
!python -m spacy download en_core_web_sm

import requests
from lxml import html
import spacy
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sqlalchemy import create_engine, Column, Integer, String, Text, JSON
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import gradio as gr

# Function to fetch article text
def fetch_article(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # Check for HTTP request errors
        tree = html.fromstring(response.content)
        # Extract all paragraphs to form the article text
        article_text = " ".join(tree.xpath('//p/text()'))
        return article_text
    except Exception as e:
        print(f"Error fetching article: {e}")
        return ""  # Return an empty string if there's an error

# Test the fetch_article function
url = "https://theprint.in/politics/modi-tears-into-congress-in-lok-sabha-attacks-nehru-indira-rajiv-rahul-for-undermining-constitution/2403747/"
article_text = fetch_article(url)
print(article_text)

# Load SpaCy NLP model
nlp = spacy.load("en_core_web_sm")

# Function to extract named entities
def extract_entities(text):
    if not text:
        return {"PERSON": [], "ORG": []}

    doc = nlp(text)
    entities = {"PERSON": [], "ORG": []}

    for ent in doc.ents:
        if ent.label_ == "PERSON":
            entities["PERSON"].append(ent.text)
        elif ent.label_ == "ORG":
            entities["ORG"].append(ent.text)

    # Remove duplicates
    entities["PERSON"] = list(set(entities["PERSON"]))
    entities["ORG"] = list(set(entities["ORG"]))
    return entities

# Test the extract_entities function
entities = extract_entities(article_text)
print("Extracted Entities:", entities)

# Initialize sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

# Function to analyze sentiment
def analyze_sentiment(text):
    if not text:
        return "Neutral"

    score = analyzer.polarity_scores(text)
    if score['compound'] > 0.05:
        return "Positive"
    elif score['compound'] < -0.05:
        return "Negative"
    else:
        return "Neutral"

# Test the analyze_sentiment function
sentiment = analyze_sentiment(article_text)
print("Sentiment:", sentiment)

# Setup SQLite database
engine = create_engine('sqlite:///articles.db')
Base = declarative_base()

# Define Article table
class Article(Base):
    __tablename__ = 'articles'
    id = Column(Integer, primary_key=True)
    url = Column(String, unique=True, nullable=False)
    text = Column(Text, nullable=False)
    entities = Column(JSON, nullable=False)
    sentiment = Column(String, nullable=False)

Base.metadata.create_all(engine)

# Create a session
Session = sessionmaker(bind=engine)
session = Session()

# Function to save article data to the database
def save_to_database(url, text, entities, sentiment):
    try:
        article = Article(url=url, text=text, entities=entities, sentiment=sentiment)
        session.add(article)
        session.commit()
        print("Article saved successfully!")
    except Exception as e:
        print(f"Error saving to database: {e}")

# Test the save_to_database function
save_to_database(url, article_text, entities, sentiment)

# Gradio UI function
def process_url(url):
    article_text = fetch_article(url)
    if not article_text:
        return "Error fetching article content", {}, "Error"

    entities = extract_entities(article_text)
    sentiment = analyze_sentiment(article_text)

    return article_text, entities, sentiment

# Define the Gradio interface
iface = gr.Interface(
    fn=process_url,
    inputs=gr.Textbox(label="Enter URL", placeholder="Enter article URL here..."),
    outputs=[
        gr.Textbox(label="Article Text"),
        gr.JSON(label="Entities (Person & Organization)"),
        gr.Textbox(label="Sentiment")
    ],
    title="Article Analyzer",
    description="Enter a URL to fetch an article and analyze its content for entities and sentiment."
)

# Launch the interface
iface.launch(share=True)